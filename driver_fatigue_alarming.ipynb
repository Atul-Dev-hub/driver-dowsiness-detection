{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "632d4207",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in .\\venv\\Lib\\site-packages (2.20.0)\n",
      "Requirement already satisfied: opencv-python in .\\venv\\Lib\\site-packages (4.13.0.90)\n",
      "Requirement already satisfied: matplotlib in .\\venv\\Lib\\site-packages (3.10.8)\n",
      "Requirement already satisfied: numpy in .\\venv\\Lib\\site-packages (2.4.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in .\\venv\\Lib\\site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in .\\venv\\Lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in .\\venv\\Lib\\site-packages (from tensorflow) (25.12.19)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in .\\venv\\Lib\\site-packages (from tensorflow) (0.7.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in .\\venv\\Lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in .\\venv\\Lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in .\\venv\\Lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in .\\venv\\Lib\\site-packages (from tensorflow) (26.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in .\\venv\\Lib\\site-packages (from tensorflow) (6.33.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in .\\venv\\Lib\\site-packages (from tensorflow) (2.32.5)\n",
      "Requirement already satisfied: setuptools in .\\venv\\Lib\\site-packages (from tensorflow) (80.10.2)\n",
      "Requirement already satisfied: six>=1.12.0 in .\\venv\\Lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in .\\venv\\Lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in .\\venv\\Lib\\site-packages (from tensorflow) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in .\\venv\\Lib\\site-packages (from tensorflow) (2.1.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in .\\venv\\Lib\\site-packages (from tensorflow) (1.76.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in .\\venv\\Lib\\site-packages (from tensorflow) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in .\\venv\\Lib\\site-packages (from tensorflow) (3.13.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in .\\venv\\Lib\\site-packages (from tensorflow) (3.15.1)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in .\\venv\\Lib\\site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in .\\venv\\Lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in .\\venv\\Lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in .\\venv\\Lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in .\\venv\\Lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2026.1.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in .\\venv\\Lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.10.1)\n",
      "Requirement already satisfied: pillow in .\\venv\\Lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (12.1.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in .\\venv\\Lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in .\\venv\\Lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in .\\venv\\Lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in .\\venv\\Lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in .\\venv\\Lib\\site-packages (from matplotlib) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in .\\venv\\Lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: pyparsing>=3 in .\\venv\\Lib\\site-packages (from matplotlib) (3.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in .\\venv\\Lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in .\\venv\\Lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.46.3)\n",
      "Requirement already satisfied: rich in .\\venv\\Lib\\site-packages (from keras>=3.10.0->tensorflow) (14.3.2)\n",
      "Requirement already satisfied: namex in .\\venv\\Lib\\site-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in .\\venv\\Lib\\site-packages (from keras>=3.10.0->tensorflow) (0.18.0)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in .\\venv\\Lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in .\\venv\\Lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in .\\venv\\Lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in .\\venv\\Lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow opencv-python matplotlib numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c095647",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d14dabdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78142ed3",
   "metadata": {},
   "source": [
    "### image preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "806dbf0a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'dataset\\\\train\\\\Closed_Eyes'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     19\u001b[39m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e: \n\u001b[32m     20\u001b[39m                 \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[43mcreate_training_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mcreate_training_data\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      8\u001b[39m path = os.path.join(DataSetDirectory, category) \u001b[38;5;66;03m# create path to Closed_Eyes and Open_Eyes\u001b[39;00m\n\u001b[32m      9\u001b[39m class_label = Classes.index(category) \u001b[38;5;66;03m# get the classification  (0 or 1). 0=Closed_Eyes 1=Open_Eyes\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     12\u001b[39m         img_array = cv2.imread(os.path.join(path, img), cv2.IMREAD_COLOR)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [WinError 3] The system cannot find the path specified: 'dataset\\\\train\\\\Closed_Eyes'"
     ]
    }
   ],
   "source": [
    "DataSetDirectory = r\"dataset\\train\"\n",
    "Classes = [\"Closed_Eyes\", \"Open_Eyes\"]\n",
    "img_size = 224 # mobilenet takes (224,224,3) images as input\n",
    "training_data = []\n",
    "\n",
    "def create_training_data():\n",
    "    for category in Classes:\n",
    "        path = os.path.join(DataSetDirectory, category) # create path to Closed_Eyes and Open_Eyes\n",
    "        class_label = Classes.index(category) # get the classification  (0 or 1). 0=Closed_Eyes 1=Open_Eyes\n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                img_array = cv2.imread(os.path.join(path, img), cv2.IMREAD_COLOR)\n",
    "                    # The cv2.IMREAD_COLOR flag ensures the image is read in BGR mode by default (3 channel).\n",
    "                    # The result is a 3D NumPy array (img_array) where each pixel value represents intensity (0 for black, 255 for white).\n",
    "                backtorgb = cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB)\n",
    "                    # Converts the 3-channel BGR image (img_array) into a 3-channel RGB image.\n",
    "                new_array = cv2.resize(backtorgb, (img_size, img_size))\n",
    "                training_data.append([new_array, class_label])\n",
    "            except Exception as e: \n",
    "                pass\n",
    "            \n",
    "create_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c19e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(training_data))\n",
    "print(training_data[0][0].shape)\n",
    "\n",
    "img = training_data[1200][0]\n",
    "plt.imshow(img) \n",
    "    # cmap: color map only used for 2D arrays (grayscale images)\n",
    "    # plt.imshow() takes array \n",
    "    # cv2.imread() takes file path (not array)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6749a306",
   "metadata": {},
   "source": [
    "### Split features and labels in X and Y (Numpy Array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17331d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "random.shuffle(training_data) # shuffle the data so that model does not learn any pattern\n",
    "\n",
    "x = [] # features\n",
    "y = [] # labels\n",
    "\n",
    "for features, label in training_data:\n",
    "    x.append(features)\n",
    "    y.append(label)\n",
    "\n",
    "x = np.array(x).reshape(-1, img_size, img_size, 3) \n",
    "    # -1 means numpy will automatically calculate the first dimension of array, which is number of samples\n",
    "    # img_size, img_size are height and width of image\n",
    "    # 3 is number of channels (RGB)\n",
    "y = np.array(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ec5838",
   "metadata": {},
   "source": [
    "### Normalisation (MinMax Scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b812e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f44f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "print(\"Before Normalization !\")\n",
    "print(x[1200])\n",
    "x_temp = x\n",
    "\n",
    "# Flatten the data to 2D (samples, features) for scaling as MinMaxScaler & StandardScaler work on 2D data\n",
    "x_flat = x.reshape(-1, img_size * img_size * 3)\n",
    "scaler = MinMaxScaler()\n",
    "x_scaled = scaler.fit_transform(x_flat)\n",
    "\n",
    "# Reshape back to original dimensions\n",
    "x = x_scaled.reshape(-1, img_size, img_size, 3)\n",
    "\n",
    "print(\"\\nAfter Normalization !\")\n",
    "print(x[1200])\n",
    "\n",
    "# -------------- It is similar to dividing by 255.0, all pixel values come under [0-1] ----------------\n",
    "# x_temp = x_temp/255.0 # normalize the data\n",
    "# print(x_temp[1600])\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "print(\"\\nShape of input features !\")\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d83d16",
   "metadata": {},
   "source": [
    "### Store the normalized dataset so that no need to run from the cell 1: pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3114c8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a982cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open(\"x.pickle\", \"wb\")\n",
    "pickle.dump(x, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"y.pickle\", \"wb\")\n",
    "pickle.dump(y, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fa7589",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(\"x.pickle\", \"rb\")\n",
    "x = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"y.pickle\", \"rb\")\n",
    "y = pickle.load(pickle_in)\n",
    "\n",
    "    # It saves the normalized data so that no need to run from the start again and again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f6aa33",
   "metadata": {},
   "source": [
    "### Training via Transfer learning using MobileNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e333a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.applications.mobilenet.MobileNet()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d728dc38",
   "metadata": {},
   "source": [
    "### Freeze layers up to and including the dropout layer\n",
    "```\n",
    "freeze = True  # Flag to control freezing\n",
    "\n",
    "for layer in model.layers:\n",
    "    layer.trainable = not freeze  # Freeze layers before the dropout layer\n",
    "    if layer.name == \"dropout\":  # Check if the current layer is the dropout layer\n",
    "        freeze = False  # Stop freezing after the dropout layer\n",
    "        \n",
    "```\n",
    "\n",
    "### Freeze layers up to and including the dropout layer (index -4)\n",
    "```\n",
    "for i, layer in enumerate(model.layers):\n",
    "    if i <= len(model.layers) - 4:  # Freeze layers up to index -4\n",
    "        layer.trainable = False\n",
    "    else:\n",
    "        layer.trainable = True\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcc442a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = tf.keras.applications.mobilenet.MobileNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03807f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tmp.layers))\n",
    "# cnt = 0\n",
    "# for _ in tmp.layers:\n",
    "#     cnt += 1\n",
    "# print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfeb811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze the last few layers of MobileNet for fine-tuning\n",
    "for layer in model.layers[:-6]:  # Freeze all layers except the last 6 out of total (91) layers \n",
    "    layer.trainable = False\n",
    "\n",
    "for layer in model.layers[-6:]:  # Unfreeze the last 6 layers\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9c9600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add dropout and L2 regularization to the custom layers\n",
    "# To reduce overfitting and improve generalization\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "base_input = model.input\n",
    "    # Retrieves the input layer of the pre-trained MobileNet model.\n",
    "    # model.input refers to the first layer of the MobileNet model, which is the input layer.\n",
    "    # base_input will be used as the input for the new model being created.\n",
    "base_output = model.layers[-4].output\n",
    "    # model.layers[-4] refers to the layer that is four layers before the last layer in the MobileNet model.\n",
    "    # base_output will be used as the starting point for adding new layers to the model.\n",
    "\n",
    "flat_layer = tf.keras.layers.Flatten(name=\"flatten\")(base_output)\n",
    "    # Adds a flattening layer to the model.\n",
    "    # The Flatten layer converts the multi-dimensional output of the previous layer (base_output) into a 1D vector.\n",
    "    # This is necessary because fully connected (dense) layers require 1D input.\n",
    "    \n",
    "# Add a dropout layer to reduce overfitting\n",
    "dropout_layer = tf.keras.layers.Dropout(0.5, name=\"dropout_1\")(flat_layer)\n",
    "\n",
    "# Add a dense layer with L2 regularization\n",
    "dense_layer = tf.keras.layers.Dense(\n",
    "    1, \n",
    "    kernel_regularizer=regularizers.l2(0.01),  # L2 regularization\n",
    "    name=\"dense_1\"\n",
    ")(dropout_layer)\n",
    "    # Adds a dense (fully connected) layer with 1 neuron.\n",
    "    # The 1 indicates that the layer has a single output, which is suitable for binary classification (e.g., \"Closed_Eyes\" vs. \"Open_Eyes\").\n",
    "    \n",
    "final_output = tf.keras.layers.Activation('sigmoid', name=\"sigmoid_output\")(dense_layer)\n",
    "    # Applies the sigmoid activation function to the output of the dense layer.\n",
    "    # The sigmoid function squashes the output to a range between 0 and 1, making it interpretable as a probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4ef5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = tf.keras.Model(inputs=base_input, outputs=final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300c3e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "new_model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# new_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# optimizers let the model train faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1817d4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17d186a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.fit(x, y, batch_size=10, epochs=4, validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e937699",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.save('my_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8470fd28",
   "metadata": {},
   "source": [
    "### Testing on testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c330c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = tf.keras.models.load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f083bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_directory = r\"dataset\\test\"  # Update this path to your testing dataset\n",
    "img_size = 224  # Same as used during training\n",
    "classes = [\"Closed_Eyes\", \"Open_Eyes\"]  # Same class labels as training\n",
    "\n",
    "# Preprocess the testing data\n",
    "testing_data = []\n",
    "\n",
    "def preprocess_testing_data():\n",
    "    for category in classes:\n",
    "        path = os.path.join(test_data_directory, category)\n",
    "        class_label = classes.index(category)\n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                img_array = cv2.imread(os.path.join(path, img), cv2.IMREAD_COLOR)\n",
    "                backtorgb = cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB)  # Same as training\n",
    "                new_array = cv2.resize(backtorgb, (img_size, img_size))\n",
    "                testing_data.append([new_array, class_label])\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "preprocess_testing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb6232b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed607390",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "for features, label in testing_data:\n",
    "    x_test.append(features)\n",
    "    y_test.append(label)\n",
    "\n",
    "x_test = np.array(x_test).reshape(-1, img_size, img_size, 3)  # Reshape to match input shape\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70a66f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291b4424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisation of test data\n",
    "x_test_flat = x_test.reshape(-1, img_size * img_size * 3)\n",
    "scaler = MinMaxScaler()\n",
    "x_test_scaled = scaler.fit_transform(x_test_flat)\n",
    "\n",
    "# Reshape back to original dimensions\n",
    "x_test = x_test_scaled.reshape(-1, img_size, img_size, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d59a7cd",
   "metadata": {},
   "source": [
    "### Save normalised testing data via pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e569cc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open(\"x_test.pickle\", \"wb\")\n",
    "pickle.dump(x_test, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"y_test.pickle\", \"wb\")\n",
    "pickle.dump(y_test, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde36148",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pickle_in = open(\"x_test.pickle\", \"rb\")\n",
    "x_test = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"y_test.pickle\", \"rb\")\n",
    "y_test = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb8e6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on testing data\n",
    "test_loss, test_accuracy = new_model.evaluate(x_test, y_test, batch_size=25, verbose=1)\n",
    "print(f\"Testing Loss: {test_loss}\")\n",
    "print(f\"Testing Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd8a76c",
   "metadata": {},
   "source": [
    "### Prediction demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4937c0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_img = cv2.imread(r\"dataset\\test\\Closed_Eyes\\_112.jpg\", cv2.IMREAD_COLOR)\n",
    "temp_backtorgb = cv2.cvtColor(temp_img, cv2.COLOR_BGR2RGB)\n",
    "temp_img_array = cv2.resize(temp_backtorgb, (img_size, img_size))\n",
    "temp_array = np.array(temp_img_array).reshape(1, img_size, img_size, 3)\n",
    "temp_array = temp_array/255.0  # Normalize the image\n",
    "plt.imshow(temp_img_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fda3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_prediction = new_model.predict(temp_array)\n",
    "print(temp_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad91d8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_img = cv2.imread(r\"dataset\\test\\Open_Eyes\\_112.jpg\", cv2.IMREAD_COLOR)\n",
    "temp_backtorgb = cv2.cvtColor(temp_img, cv2.COLOR_BGR2RGB)\n",
    "temp_img_array = cv2.resize(temp_backtorgb, (img_size, img_size))\n",
    "temp_array = np.array(temp_img_array).reshape(1, img_size, img_size, 3)\n",
    "temp_array = temp_array/255.0  # Normalize the image\n",
    "plt.imshow(temp_img_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0de9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = new_model.predict(temp_array)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f2296c",
   "metadata": {},
   "source": [
    "### Prediction on Unknown image: Fetching eyes using Haar Cascade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3061d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "women_img = cv2.imread(r\"sad_women_open_eye2.webp\", cv2.IMREAD_COLOR)\n",
    "    # cv2 reads image in BGR by deafault\n",
    "women_img = cv2.cvtColor(women_img, cv2.COLOR_BGR2RGB)\n",
    "print(women_img.shape)\n",
    "plt.imshow(women_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6797d67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8e8e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "eyeCascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc7dcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the image to grayscale for better detection\n",
    "gray_women_img = cv2.cvtColor(women_img, cv2.COLOR_RGB2GRAY)\n",
    "    # coz, Haar Cascade works better on grayscale images\n",
    "\n",
    "# Detect eyes in the grayscale image\n",
    "eyes = eyeCascade.detectMultiScale(gray_women_img, scaleFactor=1.1, minNeighbors=4)\n",
    "    # scaleFactor\n",
    "    # Haar Cascade works by scanning the image at multiple scales (zoom levels). \n",
    "    # It starts with the original image size and gradually reduces the size of the image to detect objects of different sizes.\n",
    "    # If scaleFactor=1.1, the image size is reduced by 10% at each step. For example:\n",
    "    \n",
    "    # minNeighbors:\n",
    "    # Haar Cascade works by sliding a detection window over the image and identifying objects. \n",
    "    # It often detects the same object multiple times in slightly different positions or sizes. \n",
    "    # These overlapping detections are grouped into a single detection if they are close enough.\n",
    "    # If minNeighbors=5, it means that at least 5 overlapping detections must agree for the object to be considered valid.\n",
    "    \n",
    "    # minSize\n",
    "    # minSize=(30, 30): Only objects larger than 30x30 pixels are considered.\n",
    "    # If not mentioned, then it detects objects of all size. (Here object is eye)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39cbbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if any eyes are detected\n",
    "if len(eyes) == 0:\n",
    "    print(\"No eyes detected.\")\n",
    "else:\n",
    "    # Draw rectangles around detected eyes\n",
    "    for (x, y, w, h) in eyes:\n",
    "        cv2.rectangle(women_img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            # for (x, y, w, h) in eyes:\n",
    "                # Iterates through all detected eyes. Each detection is represented as a rectangle with:\n",
    "                    # (x, y): Top-left corner of the rectangle.\n",
    "                    # (w, h): Width and height of the rectangle.\n",
    "                    \n",
    "                    # cv2.rectangle():\n",
    "                        # Draws a rectangle around the detected eyes.\n",
    "                        # Parameters:\n",
    "                            # women_img: The image on which the rectangle is drawn.\n",
    "                            # (x, y): Top-left corner of the rectangle.\n",
    "                            # (x + w, y + h): Bottom-right corner of the rectangle.\n",
    "                            # (0, 255, 0): The color of the rectangle in RGB (green in this case).\n",
    "                            # 2: The thickness of the rectangle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcea23aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the image with rectangles\n",
    "plt.imshow(women_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4054833",
   "metadata": {},
   "source": [
    "### Cropping the eyes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ea7b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y, w, h in eyes:\n",
    "    roi_gray = gray_women_img[y:y+h, x:x+w]\n",
    "    roi_color = women_img[y:y+h, x:x+w]\n",
    "        # roi: region of interest :)\n",
    "    eyes_in_roi_gray = eyeCascade.detectMultiScale(roi_gray)\n",
    "    \n",
    "    if len(eyes_in_roi_gray) == 0:\n",
    "        print(\"Eyes are not detected\")\n",
    "    else:\n",
    "        for (ex, ey, ew, eh) in eyes_in_roi_gray:\n",
    "            eye_roi = roi_color[ey:ey+eh, ex:ex+ew]\n",
    "            plt.imshow(eye_roi)\n",
    "            plt.show()\n",
    "            \n",
    "'''\n",
    "for x, y, w, h in eyes:\n",
    "\n",
    "The eyes variable contains a list of rectangles, where each rectangle represents a detected eye.\n",
    "Each rectangle is defined by:\n",
    "(x, y): Top-left corner of the rectangle.\n",
    "(w, h): Width and height of the rectangle.\n",
    "\n",
    "roi_gray:\n",
    "\n",
    "This extracts the region of interest (ROI) for the detected eye from the grayscale image (gray_women_img).\n",
    "The ROI is a cropped portion of the image containing the detected eye.\n",
    "\n",
    "roi_color:\n",
    "\n",
    "This extracts the same region of interest (ROI) for the detected eye from the original RGB image (women_img).\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "eyes_in_roi_gray = eyeCascade.detectMultiScale(roi_gray)\n",
    "\n",
    "This line applies the eye cascade again to the cropped grayscale ROI (roi_gray).\n",
    "It attempts to detect smaller eyes within the cropped region. This is useful for refining the \n",
    "detection or handling cases where the initial detection was too broad.\n",
    "'''\n",
    "\n",
    "\n",
    "'''for (ex, ey, ew, eh) in eyes_in_roi_gray:\n",
    "eye_roi:\n",
    "This extracts the final cropped eye from the colored ROI (roi_color).\n",
    "(ex, ey, ew, eh) are the coordinates of the detected eye within the ROI.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83ca266",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(eye_roi)\n",
    "    # predicting the last stored eye !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4145f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "eye_roi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9568dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_img = cv2.resize(eye_roi, (img_size, img_size))\n",
    "plt.imshow(final_img)\n",
    "print(final_img.shape)\n",
    "# final_img = np.array(final_img).reshape(1, img_size, img_size, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92086d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_img = np.array(final_img).reshape(1, img_size, img_size, 3)\n",
    "final_img = final_img/255.0  # Normalize the image\n",
    "prediction = new_model.predict(final_img)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4432863",
   "metadata": {},
   "source": [
    "### Live Video Demo of Drowsiness alert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e47b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import winsound\n",
    "# Import the winsound module for playing warning sounds\n",
    "\n",
    "frequency = 2500\n",
    "# Set the beep sound frequency to 2500 Hertz, which is high-pitched and noticeable\n",
    "\n",
    "duration = 1000\n",
    "# Set the duration of the beep sound to 1000 ms (1 second)\n",
    "\n",
    "# Create a CascadeClassifier object for face detection using the XML file\n",
    "faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Start video capture from the webcam; '1' may be replaced with '0' for default webcam\n",
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "# Check if the webcam is successfully opened\n",
    "if not cap.isOpened():\n",
    "    cap = cv2.VideoCapture(0) \n",
    "    # Try opening the default camera if previous failed\n",
    "\n",
    "if not cap.isOpened():\n",
    "    raise IOError(\"Cannot open webcam\")\n",
    "    # If still unable to open, raise an error and exit\n",
    "\n",
    "counter = 0\n",
    "# Initialize a counter variable to keep track of consecutive closed-eye frames\n",
    "\n",
    "while True:\n",
    "    # Continuously read video frames in an infinite loop\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    # Read one frame from the webcam; 'ret' is True if successful\n",
    "\n",
    "    # Haarcascade for eye detection with eyeglasses support\n",
    "    eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye_tree_eyeglasses.xml')\n",
    "\n",
    "    # Convert the frame to grayscale for easier detection processing\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces in the grayscale frame\n",
    "    faces = faceCascade.detectMultiScale(gray, 1.1, 4)\n",
    "\n",
    "    # Draw rectangles around detected faces\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        # Draw a green rectangle; thickness of 2 pixels\n",
    "\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    # Set the font type for text to be displayed on the video\n",
    "\n",
    "    # Detect eyes using the eye cascade\n",
    "    eyes = eye_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "\n",
    "    # Check if eyes are detected\n",
    "    if len(eyes) == 0:\n",
    "        print(\"eyes are not detected\")\n",
    "        # Print a message when no eyes are detected\n",
    "    else:\n",
    "        # For every detected eye, select region of interest and process\n",
    "        for (ex, ey, ew, eh) in eyes:\n",
    "            eyes_roi = frame[ey:ey+eh, ex:ex+ew]\n",
    "            # Extract the eye region from the frame\n",
    "\n",
    "            # Resize the eye region to 224x224 pixels for model input\n",
    "            final_image = cv2.resize(eyes_roi, (224, 224))\n",
    "\n",
    "            # Add a new axis to make the image batch-compliant for model prediction\n",
    "            # final_image = np.expand_dims(final_image, axis=0)\n",
    "            final_image = np.array(final_image).reshape(1, 224, 224, 3)\n",
    "\n",
    "            # Normalize pixel values to range [0, 1] for model compatibility\n",
    "            final_image = final_image / 255.0\n",
    "\n",
    "            # Predict eye state (open/closed) using a trained model\n",
    "            Predictions = new_model.predict(final_image)\n",
    "            \n",
    "            # If the eyes are open according to model prediction\n",
    "            if Predictions > 0.5:\n",
    "                status = \"Open Eyes\"\n",
    "                # Set status to indicate eyes are open\n",
    "\n",
    "                cv2.putText(\n",
    "                    frame, status, (150, 150), font, 3, (0, 255, 0), 2, cv2.LINE_4\n",
    "                )\n",
    "                # Display 'Open Eyes' in green at position (150,150)\n",
    "\n",
    "                x1, y1, w1, h1 = 0, 0, 175, 75\n",
    "                # Coordinates for a black rectangle background for text\n",
    "\n",
    "                # Draw black rectangle for text background\n",
    "                cv2.rectangle(frame, (x1, y1), (x1 + w1, y1 + h1), (0, 0, 0), -1)\n",
    "            else:\n",
    "                counter = counter + 1\n",
    "                # Increment the counter for closed-eye frames\n",
    "\n",
    "                status = \"Closed Eyes\"\n",
    "                # Set status to indicate eyes are closed\n",
    "\n",
    "                cv2.putText(\n",
    "                    frame, status, (150, 150), font, 3, (0, 0, 255), 2, cv2.LINE_4\n",
    "                )\n",
    "                # Display 'Closed Eyes' in red at position (150,150)\n",
    "\n",
    "                # Draw red rectangle around detected eyes\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "\n",
    "                # If closed-eye counter exceeds threshold (e.g., 5), trigger sleep alert\n",
    "                if counter > 5:\n",
    "                    x1, y1, w1, h1 = 0, 0, 175, 75\n",
    "                    # Coordinates for alert background\n",
    "\n",
    "                    # Draw black rectangle for alert text background\n",
    "                    cv2.rectangle(frame, (x1, y1), (x1 + w1, y1 + h1), (0, 0, 0), -1)\n",
    "\n",
    "                    # Display sleep alert text on video frame\n",
    "                    cv2.putText(\n",
    "                        frame,\n",
    "                        'Sleep Alert !!',\n",
    "                        (x1 + int(w1 / 10), y1 + int(h1 / 2)),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        0.7,\n",
    "                        (0, 0, 255),\n",
    "                        2\n",
    "                    )\n",
    "\n",
    "                    # Play warning beep sound\n",
    "                    winsound.Beep(frequency, duration)\n",
    "                    # Reset counter after alert\n",
    "                    counter = 0\n",
    "\n",
    "    # Display the video frame with all drawn rectangles and text\n",
    "    cv2.imshow('Drowsiness Detection Tutorial', frame)\n",
    "\n",
    "    # Wait for 2 milliseconds; if 'q' key is pressed, exit loop\n",
    "    if cv2.waitKey(2) & 0xFF == ord('q'):\n",
    "        break\n",
    "        # Loop ends if user presses 'q'\n",
    "\n",
    "# Release webcam and destroy all OpenCV windows after loop ends\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89447bdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Drowsiness Venv)",
   "language": "python",
   "name": "drowsiness-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
